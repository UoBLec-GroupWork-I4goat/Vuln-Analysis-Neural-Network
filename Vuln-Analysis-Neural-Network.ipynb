{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d13e1fda29bb419",
   "metadata": {},
   "source": [
    "# Javascript dependencies Vulnerabilities Analysis\n",
    "\n",
    "This is one "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03d1ea03d12ea1",
   "metadata": {},
   "source": [
    "## Dependencies of Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "d417e481e7eb9637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:14:41.310343Z",
     "start_time": "2024-10-23T13:14:41.299904Z"
    }
   },
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "import imblearn.over_sampling\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import combinations\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b096d112b2ad12d1",
   "metadata": {},
   "source": [
    "## Neural Network Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "28d798701dcde822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:14:41.353007Z",
     "start_time": "2024-10-23T13:14:41.329314Z"
    }
   },
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    # initialise the neural network\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # set the number of nodes in each input, hidden, output layer\n",
    "        self.inodes = input_nodes\n",
    "        self.hnodes = hidden_nodes\n",
    "        self.onodes = output_nodes\n",
    "\n",
    "        # link weight matrices, wih and who. \n",
    "        # Weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # activation function is the sigmoid function\n",
    "        # self.activation_function = lambda x:scipy.special.expit(x)\n",
    "        # activation function is the relu() function\n",
    "        self.activation_function = lambda x: np.maximum(0, x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "\n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "\n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "\n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "\n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        return final_outputs\n",
    "\n",
    "class NeuralNetworkFactory:\n",
    "\n",
    "    def __init__(self, seedconfig):\n",
    "        self.inputNodes = seedconfig[\"InputNodes\"]\n",
    "        self.hiddenNodes = seedconfig[\"HiddenNodes\"]\n",
    "        self.outputNodes = seedconfig['OutputNodes']\n",
    "        self.learningRate = seedconfig['LearningRate']\n",
    "        self.epochs = seedconfig['epochs']\n",
    "        self.attcombination = seedconfig['attcombination']\n",
    "        self.balancingType = seedconfig['BalancingType']\n",
    "\n",
    "    def getInputOutput(self, row_):\n",
    "\n",
    "        # split the record by the ',' commas\n",
    "        row = row_.split(',')\n",
    "\n",
    "        if(row[len(row)-1] == '\\n'):\n",
    "            return None\n",
    "\n",
    "        outputGroundTruth = float(row[len(row)-1])\n",
    "        negative = 0.01\n",
    "        positive = 0.01\n",
    "        if(outputGroundTruth == 0):\n",
    "            negative = 0.99\n",
    "        elif(outputGroundTruth ==1):\n",
    "            positive = 0.99\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        inputs = []\n",
    "        has_a = False\n",
    "        has_b = False\n",
    "        has_c = False\n",
    "        has_d = False\n",
    "\n",
    "        a = -1\n",
    "        b = -1\n",
    "        c = -1\n",
    "        d = -1\n",
    "\n",
    "\n",
    "        if(\"A\" in self.attcombination):\n",
    "            has_a = True\n",
    "            a = float(row[1]) # \n",
    "            inputs.append(a)\n",
    "        if(\"B\" in self.attcombination):\n",
    "            has_b = True\n",
    "            if(has_a):\n",
    "                b = float(row[2]) # \n",
    "            else:\n",
    "                b = float(row[1]) # \n",
    "            inputs.append(b)\n",
    "\n",
    "        if(\"C\" in self.attcombination):\n",
    "            has_c = True\n",
    "            if(has_a and has_b):\n",
    "                c = float(row[3]) # \n",
    "            elif(has_a and not has_b):\n",
    "                c = float(row[2]) # \n",
    "            elif(not has_a and has_b):\n",
    "                c = float(row[2]) # \n",
    "            elif(not has_a and not has_b):\n",
    "                c = float(row[1]) # \n",
    "\n",
    "            inputs.append(c)\n",
    "\n",
    "        if(\"D\" in self.attcombination):\n",
    "            has_d = True\n",
    "            if(has_a and has_b and has_c):\n",
    "                d = float(row[4]) # \n",
    "            elif(has_a and has_b and not has_c):\n",
    "                d = float(row[3]) # \n",
    "            elif(has_a and not has_b and has_c):\n",
    "                d = float(row[3]) # \n",
    "            elif(not has_a and has_b and has_c):\n",
    "                d = float(row[3]) # \n",
    "            elif(has_a and not has_b and not has_c):\n",
    "                d = float(row[2]) # \n",
    "            elif(not has_a and not has_b and has_c):\n",
    "                d = float(row[2]) # \n",
    "            elif(not has_a and has_b and not has_c):\n",
    "                d = float(row[2]) # \n",
    "            elif(not has_a and not has_b and not has_c):\n",
    "                d = float(row[1]) # \n",
    "\n",
    "            inputs.append(d)\n",
    "\n",
    "        # create target output value\n",
    "        targets = [negative,positive]\n",
    "\n",
    "        return [inputs, targets]\n",
    "\n",
    "    def execute(self,configuration):\n",
    "\n",
    "        self.balancingType = configuration['BalancingType']\n",
    "\n",
    "        self.dataProcessor = DataProcessor(self.epochs,configuration, self.balancingType,self.attcombination)\n",
    "        self.neuralNetwork = NeuralNetwork(self.inputNodes,self.hiddenNodes,self.outputNodes, self.learningRate)\n",
    "\n",
    "        self.trainingDataList = self.dataProcessor.getTrainingData()\n",
    "        # self.trainingDataList = self.dataProcessor.getCleanedTrainingData()\n",
    "\n",
    "        attmetrics= []\n",
    "        if(\"A\" in self.attcombination):\n",
    "            attmetrics.append('A') #            \n",
    "        if(\"B\" in self.attcombination):\n",
    "            attmetrics.append('B') #            \n",
    "        if(\"C\" in self.attcombination):\n",
    "            attmetrics.append('C') #            \n",
    "        if(\"D\" in self.attcombination):\n",
    "            attmetrics.append('D') #\n",
    "\n",
    "        attmetrics_str = ','.join(str(x) for x in attmetrics)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            rowcount = 0\n",
    "\n",
    "            #go through all the rows in the training data set\n",
    "            for row_ in self.trainingDataList:\n",
    "                rowcount = rowcount +1\n",
    "\n",
    "                result = self.getInputOutput(row_)\n",
    "                if(result == None):\n",
    "                    continue\n",
    "                inputs = result[0]\n",
    "                targets = result[1]\n",
    "\n",
    "                self.neuralNetwork.train(inputs, targets)\n",
    "\n",
    "                rate = str(round((rowcount/len(self.trainingDataList))*100,2))+\"%\"\n",
    "                # info = 'Training - epoch:'+str(epoch+1)+', Att:['+attmetrics_str+'], BT:'+str(self.balancingType)+', Row:'+rate\n",
    "                info = 'Training - epoch:'+str(epoch+1)+', Att:['+attmetrics_str+'], BT:'+str(self.balancingType)+', Row:'+rate\n",
    "                print(info, end=\" \", flush=True)\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        # test the neural network\n",
    "        self.testingDataList = self.dataProcessor.getTestingData()\n",
    "\n",
    "        # scorecard for how well the network performs, initially empty\n",
    "        scorecard = []\n",
    "\n",
    "        truepositive = 0\n",
    "        truenegative = 0\n",
    "        falsepositive = 0\n",
    "        falsenegative = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        truepositiverate = 0\n",
    "        falsepositiverate = 0\n",
    "        sensitivity = 0\n",
    "        specificity =0\n",
    "        accuracy = 0\n",
    "        f1score = 0\n",
    "\n",
    "        rowcount = 0\n",
    "\n",
    "        #go through all the rows in the testing data set\n",
    "        for row_ in self.testingDataList:\n",
    "            rowcount = rowcount +1\n",
    "\n",
    "            result = self.getInputOutput(row_)\n",
    "            if(result == None):\n",
    "                continue\n",
    "\n",
    "            # query the network\n",
    "            inputs = result[0]\n",
    "            correctOutput = result[1]\n",
    "            predictedOutput = self.neuralNetwork.query(inputs)\n",
    "\n",
    "            matched = False\n",
    "            if(round(correctOutput[0]) == round(predictedOutput[0][0])):\n",
    "                if(round(correctOutput[1]) == round(predictedOutput[1][0])):\n",
    "                    # network's answer matches correct answer, add 1 to scorecard\n",
    "                    scorecard.append(1)\n",
    "                    matched = True\n",
    "\n",
    "            if not (matched):\n",
    "                scorecard.append(0)\n",
    "\n",
    "            if(round(correctOutput[0]) == round(predictedOutput[0][0])):\n",
    "                truenegative = truenegative +1\n",
    "            else:\n",
    "                falsenegative = falsenegative +1\n",
    "\n",
    "            if(round(correctOutput[1]) == round(predictedOutput[1][0])):\n",
    "                truepositive = truepositive +1\n",
    "            else:\n",
    "                falsepositive = falsepositive +1\n",
    "\n",
    "            if((truepositive +falsepositive)>0):\n",
    "                precision = truepositive/(truepositive +falsepositive)\n",
    "            if((truepositive +falsenegative)>0):\n",
    "                recall = truepositive/(truepositive +falsenegative)\n",
    "            if((truepositive + falsenegative)>0):\n",
    "                truepositiverate = truepositive/(truepositive + falsenegative)\n",
    "            if((falsepositive + truenegative)>0):\n",
    "                falsepositiverate = falsepositive/(falsepositive + truenegative)\n",
    "            if((truepositive + falsenegative)>0):\n",
    "                sensitivity = truepositive/(truepositive + falsenegative)\n",
    "            if((truenegative + falsepositive)>0):\n",
    "                specificity = truenegative/(truenegative + falsepositive)\n",
    "            if((precision +recall)>0):\n",
    "                f1score = 2 *((precision*recall)/(precision +recall))\n",
    "\n",
    "            # calculate the performance score, the fraction of correct answers\n",
    "            scorecard_array = np.asarray(scorecard)\n",
    "            accuracy = scorecard_array.sum() / scorecard_array.size\n",
    "\n",
    "            rate = str(round((rowcount/len(self.testingDataList))*100,2))+\"%\"\n",
    "            f1score_round = str(round(f1score,4))\n",
    "\n",
    "            info = 'Testing - Data:[epochs:'+str(self.epochs)+', Atts:['+attmetrics_str+'] BT:'+self.balancingType+', Row:'+rate+', F1-Score:'+f1score_round\n",
    "            print(info, end=\"\\r\", flush=True)\n",
    "            pass\n",
    "\n",
    "        pass\n",
    "\n",
    "        scorecard_array = np.asarray(scorecard)\n",
    "        accuracy = scorecard_array.sum() / scorecard_array.size\n",
    "\n",
    "        performance ={\n",
    "            \"accuracy\":accuracy,\n",
    "            \"truepositive\":truepositive,\n",
    "            \"truenegative\":truenegative,\n",
    "            \"falsepositive\":falsepositive,\n",
    "            \"falsenegative\":falsenegative,\n",
    "            \"precision\":precision,\n",
    "            \"recall\":recall,\n",
    "            \"truepositiverate\":truepositiverate,\n",
    "            \"falsepositiverate\":falsepositiverate,\n",
    "            \"sensitivity\":sensitivity,\n",
    "            \"specificity\":specificity,\n",
    "            \"f1score\":f1score,\n",
    "            \"accuracy\":accuracy\n",
    "            # \"scorecard\": scorecard\n",
    "        }\n",
    "        # print (json.dumps(performance))\n",
    "\n",
    "        performance['scorecard']= scorecard\n",
    "        return performance"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # 进行神经网络变量的初始化\n",
    "    # 定义 输入层元数量，隐藏层元数量，输出层元数量，学习率，以及激活函数（默认为 relu）\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate, activation_function='relu'):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialization weight matrix\n",
    "        # 初始化权重矩阵\n",
    "        self.wih = np.random.normal(0.0, pow(self.hidden_nodes, -0.5), (self.hidden_nodes, self.input_nodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.output_nodes, -0.5), (self.output_nodes, self.hidden_nodes))\n",
    "\n",
    "        # 根据传入参数选择激活函数\n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        elif activation_function == 'relu':\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_function}\")\n"
   ],
   "id": "9eaec274fef8e03c"
  },
  {
   "cell_type": "markdown",
   "id": "70bca6d2c4d859a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63297d1588c044c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:14:41.359500Z",
     "start_time": "2024-10-23T13:14:41.353007Z"
    }
   },
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self,epoch, configuration_, _balancing_type, attcombination):\n",
    "        util = Utilities()\n",
    "\n",
    "        # mergeTrainingfiles = []\n",
    "        # mergeTestingfiles = []\n",
    "\n",
    "        self.balanced_data_file = \"data/_balanced\"+str(epoch)+\"/balanced.csv\"\n",
    "        self.headless_file = \"data/_headless\"+str(epoch)+\"/headerless.csv\"\n",
    "        self.training_data_file = \"data/_training\"+str(epoch)+\"/train.csv\"\n",
    "        self.testing_data_file = \"data/_testing\"+str(epoch)+\"/test.csv\"\n",
    "\n",
    "        #balance data (NB: Replace _sample.csv with your ground truth data from repocrawler)\n",
    "        util.balanceData(epoch,configuration_,\"grounddata/_sample.csv\",self.balanced_data_file, _balancing_type,attcombination)\n",
    "\n",
    "        #load training data for aws\n",
    "        util.removeHeader(self.balanced_data_file,self.headless_file)\n",
    "        util.splitTrainTestData(self.headless_file,self.training_data_file,self.testing_data_file)\n",
    "\n",
    "    def getTrainingData(self):\n",
    "        training_data_ = open(self.training_data_file, 'r')\n",
    "        training_data_list = training_data_.readlines()\n",
    "        training_data_.close()\n",
    "\n",
    "        return training_data_list\n",
    "\n",
    "    def getCleanedTrainingData(self):\n",
    "        self.cleanTrainingData()\n",
    "\n",
    "        training_data_ = open(self.training_clean_data_file, 'r')\n",
    "        training_data_list = training_data_.readlines()\n",
    "        training_data_.close()\n",
    "\n",
    "        return training_data_list\n",
    "\n",
    "    def getTestingData(self):\n",
    "        testing_data_ = open(self.testing_data_file, 'r')\n",
    "        testing_data_list = testing_data_.readlines()\n",
    "        testing_data_.close()\n",
    "\n",
    "        return testing_data_list"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "a06ef8b88525a0d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4adb5f2343aa214a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "3e9e1c55c552237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:14:41.380360Z",
     "start_time": "2024-10-23T13:14:41.359500Z"
    }
   },
   "source": [
    "class Utilities:\n",
    "\n",
    "    #balance dataset\n",
    "    def balanceData(self,epoch,configuration,f1_unbalanced,f2_balanced, _balancing_type,attcombination):\n",
    "        #check the class frequency using value_counts and find the class distribution ratio.\n",
    "        # print(f1_unbalanced)\n",
    "        data_unbalanced = pd.read_csv(f1_unbalanced)\n",
    "        data_unbalanced['Vulnerability_Truth'].value_counts()\n",
    "        positives = len(data_unbalanced[data_unbalanced['Vulnerability_Truth'] == 1])\n",
    "        negatives = len(data_unbalanced[data_unbalanced['Vulnerability_Truth'] == 0])\n",
    "\n",
    "        class_distribution_ratio = negatives/positives\n",
    "        info = 'class distribution ratio (Vulnerability vs NonVulnerability)- raw data:'+str(positives)+'/'+str(negatives)\n",
    "\n",
    "        prebalance = {\n",
    "            \"ClassDistributionRatio\":class_distribution_ratio,\n",
    "            \"NoVulnerability\":positives,\n",
    "            \"NoNonVulnerability\":negatives\n",
    "        }\n",
    "\n",
    "        configuration['PreBalance'] = prebalance\n",
    "\n",
    "        rows = []\n",
    "        # header = [\"A\", \"B\", \"C\", \"D\", ...,\"Vulnerability_Truth\"]\n",
    "        header = []\n",
    "        if(\"A\" in attcombination):\n",
    "            header.append(\"A\") # \n",
    "        if(\"B\" in attcombination):\n",
    "            header.append(\"B\") # \n",
    "        if(\"C\" in attcombination):\n",
    "            header.append(\"C\") # \n",
    "        if(\"D\" in attcombination):\n",
    "            header.append(\"D\") #         \n",
    "        # ...extend or delete depending on the attributes you choose to evaluate\n",
    "\n",
    "        header.append(\"Vulnerability_Truth\")\n",
    "\n",
    "        try:\n",
    "            with open(f1_unbalanced) as file_obj:\n",
    "                reader_obj = csv.DictReader(file_obj)\n",
    "\n",
    "                for row in reader_obj:\n",
    "                    vulnerabilityTruth = row[\"Vulnerability_Truth\"]\n",
    "\n",
    "                    if(vulnerabilityTruth == \"0\"):\n",
    "                        pass\n",
    "                    elif(vulnerabilityTruth == \"1\"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    a = 0\n",
    "                    b = 0\n",
    "                    c = 0\n",
    "                    d = 0\n",
    "                    # ...extend or delete depending on the attributes you choose to evaluate\n",
    "\n",
    "                    a = float(row[\"A\"])  # \n",
    "                    b = float(row[\"B\"])  # \n",
    "                    c = float(row[\"C\"])  # \n",
    "                    d = float(row[\"D\"])  # \n",
    "                    # ...extend or delete depending on the attributes you choose to evaluate\n",
    "\n",
    "                    t_input = []\n",
    "                    if(\"A\" in attcombination):\n",
    "                        t_input.append(a)\n",
    "                    if(\"B\" in attcombination):\n",
    "                        t_input.append(b)\n",
    "                    if(\"C\" in attcombination):\n",
    "                        t_input.append(c)\n",
    "                    if(\"D\" in attcombination):\n",
    "                        t_input.append(d)\n",
    "                    # ...extend or delete depending on the attributes you choose to evaluate                        \n",
    "\n",
    "                    t_input.append(vulnerabilityTruth)\n",
    "\n",
    "                    rows.append(t_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        pass\n",
    "\n",
    "        with open(\"data/_attended\"+str(epoch)+\"/data_self_attended.csv\", 'w') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow(header)\n",
    "            csv_writer.writerows(rows)\n",
    "\n",
    "        data_self_attended = pd.read_csv(\"data/_attended\"+str(epoch)+\"/data_self_attended.csv\")\n",
    "\n",
    "        data_balanced = []\n",
    "        if(_balancing_type == 'SMOTE'):\n",
    "            X = data_self_attended.drop('Vulnerability_Truth', axis =1)\n",
    "            y = data_self_attended['Vulnerability_Truth']\n",
    "\n",
    "            smote = imblearn.over_sampling.SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "            y_resampled_ = np.reshape(y_resampled,(-1, y_resampled.size)).transpose()\n",
    "\n",
    "            # print(X_resampled.shape,y_resampled_.shape)\n",
    "            # print(len(X_resampled),len(y_resampled_))\n",
    "\n",
    "            data_balanced.append(header)\n",
    "\n",
    "            for i in range(len(X_resampled)):\n",
    "                vulnerabilityTruth = y_resampled_[i][0]\n",
    "\n",
    "                u_input = []\n",
    "                if(\"A\" in attcombination):\n",
    "                    a = X_resampled['A'][i]\n",
    "                    u_input.append(a)\n",
    "                if(\"B\" in attcombination):\n",
    "                    b = X_resampled['B'][i]\n",
    "                    u_input.append(b)\n",
    "                if(\"C\" in attcombination):\n",
    "                    c = X_resampled['C'][i]\n",
    "                    u_input.append(c)\n",
    "                if(\"D\" in attcombination):\n",
    "                    d = X_resampled['D'][i]\n",
    "                    u_input.append(d)\n",
    "                # ...extend or delete depending on the attributes you choose to evaluate                        \n",
    "\n",
    "                u_input.append(vulnerabilityTruth)\n",
    "\n",
    "                data_balanced.append(u_input)\n",
    "\n",
    "            positives_ = 0\n",
    "            negatives_ = 0\n",
    "            for i in range(len(data_balanced)):\n",
    "                st = data_balanced[i][len(data_balanced[i])-1]\n",
    "                if(st == 1):\n",
    "                    positives_ = positives_+1\n",
    "                elif(st ==0):\n",
    "                    negatives_  = negatives_ +1\n",
    "\n",
    "            info = 'class distribution ratio (vulnerability vs non-vulnerability)-'+_balancing_type+':'+str(positives_)+'/'+str(negatives_)\n",
    "\n",
    "            class_distribution_ratio_ = negatives_/positives_\n",
    "\n",
    "            postbalance = {\n",
    "                \"BalancingType\":\"SMOTE\",\n",
    "                \"ClassDistributionRatio\":class_distribution_ratio_,\n",
    "                \"NoVulnerability\":positives_,\n",
    "                \"NoNonVulnerability\":negatives_\n",
    "            }\n",
    "            configuration['PostBalance'] = postbalance\n",
    "\n",
    "            # save data\n",
    "            if not os.path.exists(f2_balanced):\n",
    "                with open(f2_balanced, 'w') as file:\n",
    "                    pass\n",
    "            df = pd.DataFrame(data_balanced)\n",
    "            df.to_csv(f2_balanced, index=False, header=False)\n",
    "\n",
    "        elif(_balancing_type == 'RANDOMOVERSAMPLING'):\n",
    "            majority_class_label = None\n",
    "            minority_class_label = None\n",
    "\n",
    "            if(positives >negatives):\n",
    "                majority_class_label = 1\n",
    "                minority_class_label = 0\n",
    "            else:\n",
    "                majority_class_label = 0\n",
    "                minority_class_label = 1\n",
    "\n",
    "            minority_class = data_self_attended[data_self_attended['Vulnerability_Truth'] == minority_class_label]\n",
    "            majority_class = data_self_attended[data_self_attended['Vulnerability_Truth'] == majority_class_label]\n",
    "\n",
    "            # Upsample the minority class (random oversampling)\n",
    "            minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "            # Combine the upsampled minority class with the majority class\n",
    "            data_balanced = pd.concat([majority_class, minority_upsampled])\n",
    "            # \n",
    "            positives_ = len(data_balanced[data_balanced['Vulnerability_Truth'] == 1])\n",
    "            negatives_ = len(data_balanced[data_balanced['Vulnerability_Truth'] == 0])\n",
    "            info = 'class distribution ratio (vulnerability vs non-vulnerability)-' +_balancing_type+':'+str(positives_)+'/'+str(negatives_)\n",
    "\n",
    "            class_distribution_ratio_ = negatives_/positives_\n",
    "\n",
    "            postbalance = {\n",
    "                \"BalancingType\":\"RANDOMOVERSAMPLING\",\n",
    "                \"ClassDistributionRatio\":class_distribution_ratio_,\n",
    "                \"NoVulnerability\":positives_,\n",
    "                \"NoNonVulnerability\":negatives_\n",
    "            }\n",
    "            configuration['PostBalance'] = postbalance\n",
    "\n",
    "            # Save the data\n",
    "            # if not os.path.exists(f2_balanced): \n",
    "            #     with open(f2_balanced, 'w') as file: \n",
    "            #         pass\n",
    "\n",
    "            # with open(f2_balanced, \"w\") as f:\n",
    "            #     f.write(\"\\n\".join(str(data_balanced)))\n",
    "            if not os.path.exists(f2_balanced):\n",
    "                with open(f2_balanced, 'w') as file:\n",
    "                    pass\n",
    "            df = pd.DataFrame(data_balanced)\n",
    "            df.to_csv(f2_balanced, index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "        elif(_balancing_type == 'RANDOMUNDERSAMPLING'):\n",
    "            majority_class_label = None\n",
    "            minority_class_label = None\n",
    "\n",
    "            if(positives >negatives):\n",
    "                majority_class_label = 1\n",
    "                minority_class_label = 0\n",
    "            else:\n",
    "                majority_class_label = 0\n",
    "                minority_class_label = 1\n",
    "\n",
    "            minority_class = data_self_attended[data_self_attended['Vulnerability_Truth'] == minority_class_label]\n",
    "            majority_class = data_self_attended[data_self_attended['Vulnerability_Truth'] == majority_class_label]\n",
    "\n",
    "            # Downsample the majority class (random undersampling)\n",
    "            majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "            # Combine the downsampled majority class with the minority class\n",
    "            data_balanced = pd.concat([minority_class, majority_downsampled])\n",
    "            # \n",
    "            positives_ = len(data_balanced[data_balanced['Vulnerability_Truth'] == 1])\n",
    "            negatives_ = len(data_balanced[data_balanced['Vulnerability_Truth'] == 0])\n",
    "            info = 'class distribution ratio (vulnerability vs non-vulnerability)-' +_balancing_type+':'+str(positives_)+'/'+str(negatives_)\n",
    "\n",
    "            class_distribution_ratio_ = negatives_/positives_\n",
    "            postbalance = {\n",
    "                \"BalancingType\":\"RANDOMUNDERSAMPLING\",\n",
    "                \"ClassDistributionRatio\":class_distribution_ratio_,\n",
    "                \"NoVulnerability\":positives_,\n",
    "                \"NoNonVulnerability\":negatives_\n",
    "            }\n",
    "            configuration['PostBalance'] = postbalance\n",
    "\n",
    "            # Save the data\n",
    "            # if not os.path.exists(f2_balanced): \n",
    "            #     with open(f2_balanced, 'w') as file: \n",
    "            #         pass\n",
    "\n",
    "            # with open(f2_balanced, \"w\") as f:\n",
    "            #     f.write(\"\\n\".join(str(data_balanced)))            \n",
    "            if not os.path.exists(f2_balanced):\n",
    "                with open(f2_balanced, 'w') as file:\n",
    "                    pass\n",
    "            df = pd.DataFrame(data_balanced)\n",
    "            df.to_csv(f2_balanced, index=False, header=False)\n",
    "\n",
    "        return data_balanced\n",
    "\n",
    "    def createDirs(self, directory):\n",
    "        shutil.rmtree(directory, ignore_errors=True)\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    #read and remove headers from data\n",
    "    def removeHeader(self,f1,f2):\n",
    "        with open(f1, \"r\") as f:\n",
    "            data = f.read().split(\"\\n\")\n",
    "\n",
    "        # Remove the 1st line\n",
    "        del data[0]\n",
    "\n",
    "        # Save the data\n",
    "        if not os.path.exists(f2):\n",
    "            with open(f2, 'w') as file:\n",
    "                pass\n",
    "\n",
    "        with open(f2, \"w\") as f:\n",
    "            f.write(\"\\n\".join(data))\n",
    "\n",
    "    def splitTrainTestData(self,all_file,train_file,test_file):\n",
    "        df = pd.read_csv(all_file)\n",
    "        indices = np.arange(len(df))\n",
    "        indices_train, indices_test = train_test_split(indices, test_size=0.2)\n",
    "        df_train = df.iloc[indices_train]\n",
    "        df_test = df.iloc[indices_test]\n",
    "\n",
    "        df_train.to_csv(train_file)\n",
    "        df_test.to_csv(test_file)\n",
    "\n",
    "\n",
    "    def configexist(configuration):\n",
    "        exist = False\n",
    "\n",
    "        label1 = Utilities.getLabel(configuration)\n",
    "\n",
    "        # check completed configurations\n",
    "        performancedir = \"data/_performance\"+str(configuration[\"epochs\"])\n",
    "\n",
    "        dirExist = os.path.exists(performancedir)\n",
    "        if (dirExist):\n",
    "            pfiles = [f for f in listdir(performancedir) if isfile(join(performancedir, f))]\n",
    "\n",
    "            for pfile in pfiles:\n",
    "                configuration = Utilities.readconfigPerformance(performancedir,pfile)\n",
    "                label2 = Utilities.getLabel(configuration)\n",
    "\n",
    "                if(label1 == label2):\n",
    "                    exist = True\n",
    "                    break\n",
    "        if(exist):\n",
    "            return configuration\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def readconfigPerformance(dir, fileName):\n",
    "        configuration = None\n",
    "        with open(dir+\"/\"+fileName, 'r') as openfile:\n",
    "            # Reading from json file\n",
    "            configuration = json.load(openfile)\n",
    "        return configuration\n",
    "\n",
    "\n",
    "    def getLabel(configuration):\n",
    "        attmetrics= []\n",
    "        if(\"A\" in configuration['attcombination']):\n",
    "            attmetrics.append('A') # age            \n",
    "        if(\"B\" in configuration['attcombination']):\n",
    "            attmetrics.append('B') # number of issues         \n",
    "        if(\"C\" in configuration['attcombination']):\n",
    "            attmetrics.append('C') # difference in date between last repository update and last issue           \n",
    "        if(\"D\" in configuration['attcombination']):\n",
    "            attmetrics.append('D') # regulatory authority                 \n",
    "        # ...extend or delete depending on the attributes you choose to evaluate\n",
    "\n",
    "        attmetrics_str = ','.join(str(x) for x in attmetrics)\n",
    "\n",
    "        label ='Data:[epochs:'+str(configuration['epochs'])+', BT:'+str(configuration[\"BalancingType\"])+', ATT:['+attmetrics_str+']'\n",
    "\n",
    "        return label\n",
    "\n",
    "    def writeConfigPerformance(configuration):\n",
    "        performancedir = \"data/_performance\"+str(configuration[\"epochs\"])\n",
    "        isExist = os.path.exists(performancedir)\n",
    "        if not isExist:\n",
    "            os.makedirs(performancedir)\n",
    "        uid = Utilities.getUniqueId(configuration)\n",
    "        performance_file = \"data/_performance\"+str(configuration[\"epochs\"])+\"/p_\"+uid+\".json\"\n",
    "\n",
    "        # Serializing json\n",
    "        json_object = json.dumps(configuration, indent=2)\n",
    "\n",
    "        # Writing to performance dir\n",
    "        with open(performance_file, \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def getUniqueId(configuration):\n",
    "        attmetrics= []\n",
    "        if(\"A\" in configuration['attcombination']):\n",
    "            attmetrics.append('A') # age            \n",
    "        if(\"B\" in configuration['attcombination']):\n",
    "            attmetrics.append('B') # number of issues         \n",
    "        if(\"C\" in configuration['attcombination']):\n",
    "            attmetrics.append('C') # difference in date between last repository update and last issue           \n",
    "        if(\"D\" in configuration['attcombination']):\n",
    "            attmetrics.append('D') # regulatory authority     \n",
    "        # ...extend or delete depending on the attributes you choose to evaluate\n",
    "\n",
    "        attmetrics_str = ''.join(str(x) for x in attmetrics)\n",
    "        uid =str(configuration['epochs'])+attmetrics_str+str(configuration[\"BalancingType\"])\n",
    "\n",
    "        return uid\n",
    "\n",
    "\n",
    "    def prettyprintconfig(configuration, appendtofile):\n",
    "\n",
    "        configconvert = {\n",
    "            \"epochs\":configuration[\"epochs\"],\n",
    "            # \"BT\":configuration[\"BalancingType\"],\n",
    "            \"preBal\":str(round(configuration[\"PreBalance\"][\"ClassDistributionRatio\"],3))+\":\"+str(round(configuration[\"PreBalance\"][\"NoVulnerability\"],3))+\"/\"+str(round(configuration[\"PreBalance\"][\"NoNonVulnerability\"],3)),\n",
    "            \"postBal\":configuration[\"PostBalance\"]['BalancingType']+\"(\"+str(round(configuration[\"PostBalance\"][\"ClassDistributionRatio\"],3))+\":\"+str(round(configuration[\"PostBalance\"][\"NoVulnerability\"],3))+\"/\"+str(round(configuration[\"PostBalance\"][\"NoNonVulnerability\"],3))+\")\",\n",
    "            \"atts\":configuration[\"attcombination\"],\n",
    "            \"TPR\":round(configuration[\"performance\"][\"truepositiverate\"],3),\n",
    "            \"FPR\":round(configuration[\"performance\"][\"falsepositiverate\"],3),\n",
    "            \"P\":round(configuration[\"performance\"][\"precision\"],3),\n",
    "            \"R\":round(configuration[\"performance\"][\"recall\"],3),\n",
    "            \"A\":round(configuration[\"performance\"][\"accuracy\"],3),\n",
    "            \"F1\":round(configuration[\"performance\"][\"f1score\"],3)\n",
    "        }\n",
    "\n",
    "        print('\\r' + json.dumps(configconvert), end='')\n",
    "\n",
    "        if(appendtofile):\n",
    "            resultsfile = \"data/_results\"+str(configuration[\"epochs\"])+\"/results.json\"\n",
    "            f = open(resultsfile, \"a\")\n",
    "            f.write(json.dumps(configconvert))\n",
    "            f.close()\n",
    "\n",
    "            row = []\n",
    "            row.append(configuration[\"InputNodes\"])\n",
    "            row.append(configuration[\"LearningRate\"])\n",
    "            row.append(configuration[\"HiddenNodes\"])\n",
    "            row.append(configuration[\"OutputNodes\"])\n",
    "            row.append(configuration[\"epochs\"])\n",
    "            row.append(configuration[\"PreBalance\"][\"ClassDistributionRatio\"])\n",
    "            row.append(configuration[\"PreBalance\"][\"NoVulnerability\"])\n",
    "            row.append(configuration[\"PreBalance\"][\"NoNonVulnerability\"])\n",
    "            row.append(configuration[\"PostBalance\"]['BalancingType'])\n",
    "            row.append(configuration[\"PostBalance\"][\"ClassDistributionRatio\"])\n",
    "            row.append(configuration[\"PostBalance\"][\"NoVulnerability\"])\n",
    "            row.append(configuration[\"PostBalance\"][\"NoNonVulnerability\"])\n",
    "            row.append(configuration[\"attcombination\"])\n",
    "            row.append(configuration[\"performance\"][\"truepositiverate\"])\n",
    "            row.append(configuration[\"performance\"][\"falsepositiverate\"])\n",
    "            row.append(configuration[\"performance\"][\"precision\"])\n",
    "            row.append(configuration[\"performance\"][\"recall\"])\n",
    "            row.append(configuration[\"performance\"][\"accuracy\"])\n",
    "            row.append(configuration[\"performance\"][\"f1score\"])\n",
    "\n",
    "            with open(\"data/_results\"+str(configuration[\"epochs\"])+\"/results.csv\", 'a') as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow(row)\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "4092f7c4974ccf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "52e4875cd46df1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:14:41.385210Z",
     "start_time": "2024-10-23T13:14:41.380360Z"
    }
   },
   "source": [
    "def processInstance(epochs, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "\n",
    "    attrs = [\"A\", \"B\", \"C\", \"D\"] #selected combination of input attributes ... n  \n",
    "    # ...extend or delete depending on the attributes you choose to evaluate\n",
    "\n",
    "    balancingTypes = ['SMOTE','RANDOMOVERSAMPLING', 'RANDOMUNDERSAMPLING']\n",
    "\n",
    "    for b in range(1,len(balancingTypes)):\n",
    "        balancingType = balancingTypes[b]\n",
    "\n",
    "        for r in range(1,len(attrs)):\n",
    "            attcombinations = list(combinations(attrs, r))\n",
    "\n",
    "            for i in range(len(attcombinations)):\n",
    "                input_nodes = len(attcombinations[0])\n",
    "                attcombination = attcombinations[i]\n",
    "\n",
    "                configuration = {\n",
    "                    \"InputNodes\":input_nodes,\n",
    "                    \"HiddenNodes\":hidden_nodes,\n",
    "                    \"OutputNodes\":output_nodes,\n",
    "                    \"epochs\":epochs,\n",
    "                    \"LearningRate\":learning_rate,\n",
    "                    \"BalancingType\":balancingType,\n",
    "                    \"attcombination\":attcombination\n",
    "                }\n",
    "\n",
    "                configurationExist = Utilities.configexist(configuration)\n",
    "                if (configurationExist == None):\n",
    "                    annf = NeuralNetworkFactory(configuration)\n",
    "                    configuration['performance'] = annf.execute(configuration)\n",
    "                    Utilities.writeConfigPerformance(configuration)\n",
    "                    # Utilities.prettyprintconfig(configuration,True)\n",
    "                else:\n",
    "                    configuration = configurationExist\n",
    "                    # Utilities.prettyprintconfig(configuration,False)\n",
    "    pass\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "5918939c91ebf252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "f6ee47c6e54c3c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:21:11.436819Z",
     "start_time": "2024-10-23T13:21:10.440776Z"
    }
   },
   "source": [
    "def main():\n",
    "    # number of input, hidden and output nodes\n",
    "    input_nodes = 5\n",
    "    hidden_nodes = 6\n",
    "    output_nodes = 2\n",
    "\n",
    "    # learning rate\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    # data reuse\n",
    "    max_epochs = 10\n",
    "\n",
    "    pheader = []\n",
    "    pheader.append(\"InputNodes\")\n",
    "    pheader.append(\"LearningRate\")\n",
    "    pheader.append(\"HiddenNodes\")\n",
    "    pheader.append(\"OutputNodes\")\n",
    "    pheader.append(\"epochs\")\n",
    "    pheader.append(\"PreBalance:ClassDistributionRatio\")\n",
    "    pheader.append(\"BalancingType\")\n",
    "    pheader.append(\"PostBalance:ClassDistributionRatio\")\n",
    "    pheader.append(\"truepositiverate\")\n",
    "    pheader.append(\"falsepositiverate\")\n",
    "    pheader.append(\"precision\")\n",
    "    pheader.append(\"recall\")\n",
    "    pheader.append(\"accuracy\")\n",
    "    pheader.append(\"f1score\")\n",
    "\n",
    "    for epochs in range(1,max_epochs):\n",
    "        resultsdir = \"data/_results\"+str(epochs)\n",
    "        headlessdir = \"data/_headless\"+str(epochs)\n",
    "        trainingdir = \"data/_training\"+str(epochs)\n",
    "        testingdir = \"data/_testing\"+str(epochs)\n",
    "        balanceddir = \"data/_balanced\"+str(epochs)\n",
    "        attendeddir = \"data/_attended\"+str(epochs)\n",
    "\n",
    "        Utilities().createDirs(resultsdir)\n",
    "        Utilities().createDirs(headlessdir)\n",
    "        Utilities().createDirs(trainingdir)\n",
    "        Utilities().createDirs(testingdir)\n",
    "        Utilities().createDirs(balanceddir)\n",
    "        Utilities().createDirs(attendeddir)\n",
    "\n",
    "        with open(resultsdir+'/results.csv', 'w') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow(pheader)\n",
    "\n",
    "        # process = Process(target=processInstance, args=(epochs,input_nodes,hidden_nodes,output_nodes,learning_rate,))\n",
    "        # process.start()\n",
    "        processInstance(epochs,input_nodes,hidden_nodes,output_nodes,learning_rate)\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89f70c6ed72b72b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
